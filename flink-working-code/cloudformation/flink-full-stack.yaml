AWSTemplateFormatVersion: '2010-09-09'
Description: >
  Flink POC - Kinesis Stream + S3 Buckets + Managed Flink App
  NO IAM resources in this template.
  Uses the role ARN created by your existing flink-cf-template.yaml

# ──────────────────────────────────────────────────────────────
# IMPORTANT:
# Before deploying this template, first deploy your working
# flink-cf-template.yaml to create the IAM role.
# Then get the role ARN and pass it as FlinkRoleArn below.
#
# Get role ARN after deploying flink-cf-template.yaml:
#   aws cloudformation describe-stacks \
#     --stack-name flink-iam-stack \
#     --query "Stacks[0].Outputs[?OutputKey=='FlinkRoleArn'].OutputValue" \
#     --output text
# ──────────────────────────────────────────────────────────────

Parameters:

  ApplicationName:
    Type: String
    Default: flink-poc
    Description: Name of the Flink application

  BucketSuffix:
    Type: String
    Default: flink-poc
    Description: Lowercase suffix for S3 bucket names

  LogRetentionDays:
    Type: Number
    Default: 7
    AllowedValues: [ 1, 3, 5, 7, 14, 30, 60, 90 ]

  FlinkRoleArn:
    Type: String
    Description: >
      ARN of the IAM role created by your flink-cf-template.yaml.
      Example: arn:aws:iam::739275465799:role/svc-flink-poc-role

Resources:

  # ── CloudWatch Log Group ──────────────────────────────────────
  FlinkLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/kinesis-analytics/${ApplicationName}'
      RetentionInDays: !Ref LogRetentionDays

  FlinkLogStream:
    Type: AWS::Logs::LogStream
    Properties:
      LogGroupName: !Ref FlinkLogGroup
      LogStreamName: "flink-app-logs"

  # ── S3 Bucket for App Code and JARs ──────────────────────────
  # Upload main.py zip and JARs here
  FlinkApplicationBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${AWS::AccountId}-${BucketSuffix}-app'
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpiration:
              NoncurrentDays: 30

  # ── S3 Bucket for Flink Output ────────────────────────────────
  # Flink writes processed records here
  FlinkOutputBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${AWS::AccountId}-${BucketSuffix}-output'
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldLogs
            Status: Enabled
            ExpirationInDays: 30

  # ── Kinesis Input Stream ──────────────────────────────────────
  # Python generator sends records here → Flink reads from here
  FlinkInputKinesisStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: !Sub '${ApplicationName}-input-stream'
      ShardCount: 1
      RetentionPeriodHours: 24

  # ── AWS Managed Flink Application ─────────────────────────────
  FlinkApplication:
    Type: AWS::KinesisAnalyticsV2::Application
    DependsOn:
      - FlinkInputKinesisStream
      - FlinkApplicationBucket
      - FlinkOutputBucket
    Properties:
      ApplicationName: !Sub '${ApplicationName}-app'
      RuntimeEnvironment: "FLINK-1_18"
      # Uses role ARN from your existing flink-cf-template.yaml
      ServiceExecutionRole: !Ref FlinkRoleArn

      ApplicationConfiguration:

        FlinkApplicationConfiguration:
          ParallelismConfiguration:
            ConfigurationType: CUSTOM
            Parallelism: 1
            ParallelismPerKPU: 1
            AutoScalingEnabled: false
          CheckpointConfiguration:
            ConfigurationType: CUSTOM
            CheckpointingEnabled: true
            CheckpointInterval: 60000
            MinPauseBetweenCheckpoints: 5000
          MonitoringConfiguration:
            ConfigurationType: CUSTOM
            MetricsLevel: APPLICATION
            LogLevel: INFO

        # Python ZIP + JARs in app bucket
        ApplicationCodeConfiguration:
          CodeContent:
            S3ContentLocation:
              BucketARN: !GetAtt FlinkApplicationBucket.Arn
              FileKey: "python/flink-python-app.zip"
          CodeContentType: ZIPFILE

        EnvironmentProperties:
          PropertyGroups:

            - PropertyGroupId: "kinesis.analytics.flink.run.options"
              PropertyMap:
                python: "main.py"
                jarfile: "jars/flink-sql-connector-kinesis-4.0.0-1.16.jar;jars/flink-s3-fs-hadoop-1.18.1.jar"

            - PropertyGroupId: "ApplicationProperties"
              PropertyMap:
                input.stream.name:  !Sub '${ApplicationName}-input-stream'
                output.bucket.name: !Sub '${AWS::AccountId}-${BucketSuffix}-output'
                output.prefix:      "processed-logs"
                aws.region:         !Ref "AWS::Region"
                stream.position:    "LATEST"

  # ── CloudWatch Logging ─────────────────────────────────────────
  FlinkAppCloudWatchLogging:
    Type: AWS::KinesisAnalyticsV2::ApplicationCloudWatchLoggingOption
    DependsOn: FlinkApplication
    Properties:
      ApplicationName: !Ref FlinkApplication
      CloudWatchLoggingOption:
        LogStreamARN: !Sub >-
          arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/kinesis-analytics/${ApplicationName}:log-stream:flink-app-logs

# ──────────────────────────────────────────────────────────────
# Outputs
# ──────────────────────────────────────────────────────────────
Outputs:

  AppBucketName:
    Description: Upload your main.py zip and JARs here
    Value: !Ref FlinkApplicationBucket

  OutputBucketName:
    Description: Flink writes processed records here
    Value: !Ref FlinkOutputBucket

  InputStreamName:
    Description: Send records here from the generator
    Value: !Ref FlinkInputKinesisStream

  FlinkAppName:
    Description: Managed Flink application name
    Value: !Ref FlinkApplication

  CloudWatchLogGroup:
    Description: View Flink logs here
    Value: !Ref FlinkLogGroup
